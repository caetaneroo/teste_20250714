import asyncio
import time
import json
import logging
import re
import os
import traceback
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI

import rate_limiter
import stats_manager

logger = logging.getLogger(__name__)

BR_TIMEZONE = ZoneInfo("America/Sao_Paulo")

DEFAULT_MAX_RETRY = 3
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TPM = 1500000
DEFAULT_INITIAL_CONCURRENCY = 30  # Reduzido para inÃ­cio mais conservador
API_CALL_TIMEOUT_SECONDS = 120.0  # Aumentado para 2 minutos


@dataclass
class AIProcessorConfig:
    """ConfiguraÃ§Ã£o validada para o AIProcessor."""
    client_id: str
    client_secret: str
    model: str = "gpt-4o-mini"
    embedding_model: str = "text-embedding-ada-002"
    temperature: float = DEFAULT_TEMPERATURE
    max_tokens: Optional[int] = None
    max_tpm: int = DEFAULT_MAX_TPM
    initial_concurrency: int = DEFAULT_INITIAL_CONCURRENCY
    max_retry: int = DEFAULT_MAX_RETRY
    environment: str = "dev"
    provider: str = "azure_openai"
    correlation_id: str = "ai-processor-final"
    enable_embedding_cache: bool = False


def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega a configuraÃ§Ã£o dos modelos a partir de um arquivo JSON no diretÃ³rio do mÃ³dulo."""
    module_dir = os.path.dirname(os.path.abspath(__file__))
    full_config_path = os.path.join(module_dir, config_path)
    if not os.path.exists(full_config_path):
        raise FileNotFoundError(f"Arquivo de configuraÃ§Ã£o '{full_config_path}' nÃ£o encontrado.")
    try:
        with open(full_config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        raise RuntimeError(f"Erro ao carregar '{full_config_path}': {e}")


class JSONSchemaNotSupportedError(Exception):
    pass


class AIProcessor:
    class _BatchProgressTracker:
        def __init__(self, total: int, batch_id: str):
            self.total = total
            self.batch_id = batch_id
            self.completed_count = 0
            self.total_tokens_used = 0
            self.start_time = time.time()
            self.logged_milestones = set()
            self._lock = asyncio.Lock()

        async def increment_and_log(self, tokens_used: int = 0):
            async with self._lock:
                self.completed_count += 1
                self.total_tokens_used += tokens_used
                progress_percent = (self.completed_count / self.total) * 100
                
                current_milestone = int(progress_percent // 10) * 10
                if current_milestone > 0 and current_milestone not in self.logged_milestones:
                    self.logged_milestones.add(current_milestone)
                    elapsed = time.time() - self.start_time
                    req_rate = self.completed_count / elapsed if elapsed > 0 else 0
                    tpm_rate = (self.total_tokens_used / elapsed) * 60 if elapsed > 0 else 0
                    eta_seconds = (self.total - self.completed_count) / req_rate if req_rate > 0 else 0
                    logger.info(
                        f"ğŸ“Š Progresso do Lote '{self.batch_id}': {self.completed_count}/{self.total} ({progress_percent:.1f}%) | "
                        f"Taxa Req: {req_rate:.2f}/s | Taxa TPM: {tpm_rate:,.0f} | ETA: {eta_seconds/60:.1f} min"
                    )
        
        @property
        def remaining(self) -> int:
            return self.total - self.completed_count

    def __init__(self, config: AIProcessorConfig):
        self.models_config = load_models_config()
        self.config = config
        self.client = AsyncIaraGenAI(
            client_id=config.client_id, client_secret=config.client_secret,
            environment=config.environment, provider=config.provider, correlation_id=config.correlation_id
        )
        self.stats_manager = stats_manager.StatsManager(max_tpm=config.max_tpm)
        self.rate_limiter = rate_limiter.AdaptiveRateLimiter(
            max_tpm=config.max_tpm,
            stats_callback=self._create_stats_callback(),
            initial_concurrency=config.initial_concurrency
        )
        self.embedding_cache = {} if config.enable_embedding_cache else None
        self.batch_stats_cache = {}

    def _create_stats_callback(self) -> Callable:
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback
        
    def _get_model_pricing(self, model: str) -> Dict[str, float]:
        model_data = self.models_config.get(model, {})
        return {'input': model_data.get('input', 0.0), 'output': model_data.get('output', 0.0), 'cache': model_data.get('cache', 0.0)}

    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int = 0, cached_tokens: int = 0) -> float:
        pricing = self._get_model_pricing(model)
        regular_input = max(0, input_tokens - cached_tokens)
        return ((regular_input / 1000) * pricing['input'] + (cached_tokens / 1000) * pricing['cache'] + (output_tokens / 1000) * pricing['output'])

    def _supports_json_schema(self, model: str) -> bool:
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]], model: str):
        if json_schema and not self._supports_json_schema(model):
            supported_models = [m for m, cfg in self.models_config.items() if cfg.get('json_schema')]
            raise JSONSchemaNotSupportedError(f"Modelo '{model}' nÃ£o suporta json_schema. Modelos compatÃ­veis: {', '.join(supported_models)}")

    @staticmethod
    def _is_rate_limit_error(error_response: Dict[str, Any]) -> bool:
        """Detecta se o erro Ã© relacionado a rate limiting."""
        if not isinstance(error_response, dict):
            return False
        
        error_msg = str(error_response.get('error', '')).lower()
        error_details = error_response.get('error_details', {})
        error_type = str(error_details.get('type', '')).lower()
        
        # PadrÃµes de rate limit conhecidos
        rate_limit_patterns = [
            'rate limit',
            'token rate limit',
            'requests per minute',
            'too many requests',
            'quota exceeded',
            'rateLimitError',
            'rate_limit_exceeded'
        ]
        
        # Verifica nos diferentes campos
        text_to_check = f"{error_msg} {error_type}"
        return any(pattern in text_to_check for pattern in rate_limit_patterns)

    @staticmethod
    def _is_timeout_error(error_response: Dict[str, Any]) -> bool:
        """Detecta se o erro Ã© especificamente de timeout."""
        if not isinstance(error_response, dict):
            return False
        
        error_details = error_response.get('error_details', {})
        error_type = str(error_details.get('type', '')).lower()
        
        return error_type in ['timeouterror', 'timeout']

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        """Extrai tempo de espera de headers ou mensagem de erro."""
        # Primeiro tenta headers
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try:
                return float(headers['retry-after'])
            except (ValueError, TypeError):
                pass
        
        # Depois tenta parsing da mensagem
        error_msg = result.get('error', '').lower()
        patterns = [
            r'try again in.*?(\d+\.?\d*)\s*s',
            r'retry after (\d+\.?\d*) seconds',
            r'wait (\d+\.?\d*) seconds'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, error_msg)
            if match:
                return float(match.group(1))
        
        # Tempo padrÃ£o baseado no tipo de erro
        if 'quota' in error_msg or 'daily' in error_msg:
            return 300.0  # 5 minutos para quota diÃ¡ria
        elif 'minute' in error_msg:
            return 60.0   # 1 minuto para RPM
        else:
            return 30.0   # 30 segundos padrÃ£o

    @staticmethod
    def _normalize_ids(ids: Any, total_items: int) -> Optional[List[str]]:
        if ids is None: return None
        if hasattr(ids, 'tolist'): id_list = ids.tolist()
        elif hasattr(ids, 'values'): id_list = list(ids.values)
        elif not isinstance(ids, list): id_list = list(ids)
        else: id_list = ids
        if len(id_list) != total_items:
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(id_list)} != {total_items}")
        return [str(cid) if cid is not None else None for cid in id_list]

    def _get_current_timestamp_iso(self) -> str:
        now = time.time()
        dt_object = datetime.fromtimestamp(now, tz=BR_TIMEZONE)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]

    async def _make_api_call(self, params: Dict[str, Any], request_id: str, batch_id: Optional[str] = None, is_embedding: bool = False) -> Dict[str, Any]:
        """Faz chamada Ã  API com tratamento robusto de erros e timeout."""
        model = params.get('model', self.config.model if not is_embedding else self.config.embedding_model)
        
        await self.rate_limiter.await_permission_to_proceed(batch_id)
        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(self.config.max_retry), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    response = None
                    
                    try:
                        # Prepara a chamada da API
                        if is_embedding:
                            api_coro = self.client.embeddings.create(**params)
                        else:
                            self._validate_json_schema_compatibility(params.get('response_format'), model)
                            api_coro = self.client.chat.completions.create(**params)
                        
                        # Executa com timeout
                        response = await asyncio.wait_for(api_coro, timeout=API_CALL_TIMEOUT_SECONDS)
                        
                        # Processa resposta de sucesso
                        if is_embedding:
                            content = response.data[0].embedding if isinstance(params.get("input"), str) else [emb.embedding for emb in response.data]
                            input_tokens, output_tokens, cached_tokens = response.usage.prompt_tokens, 0, 0
                        else:
                            content = response.choices[0].message.content
                            if params.get('response_format') and content:
                                try: content = json.loads(content)
                                except json.JSONDecodeError: pass
                            input_tokens, output_tokens = response.usage.prompt_tokens, response.usage.completion_tokens
                            cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0)
                        
                        return {
                            'id': request_id, 'success': True, 'content': content, 'model': model,
                            'input_tokens': input_tokens, 'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens, 'total_tokens': input_tokens + output_tokens,
                            'tokens_for_limiter': (input_tokens - cached_tokens) + output_tokens,
                            'cost': self._calculate_cost(model, input_tokens, output_tokens, cached_tokens),
                            'api_response_time': time.time() - start_time, 'attempts': attempt.retry_state.attempt_number
                        }
                    
                    except asyncio.TimeoutError:
                        # Timeout especÃ­fico - nÃ£o confundir com rate limit
                        api_response_time = time.time() - start_time
                        is_last_attempt = attempt.retry_state.attempt_number == self.config.max_retry
                        
                        logger.warning(
                            f"â±ï¸ Timeout na API (>{API_CALL_TIMEOUT_SECONDS}s) para req {request_id}. "
                            f"Tentativa {attempt.retry_state.attempt_number}/{self.config.max_retry}. "
                            f"LatÃªncia: {api_response_time:.1f}s"
                            f"{' - Ãšltima tentativa!' if is_last_attempt else ' - Tentando novamente...'}"
                        )
                        
                        if is_last_attempt:
                            return {
                                'id': request_id, 'success': False,
                                'error': f'Timeout apÃ³s {self.config.max_retry} tentativas. Chamada excedeu {API_CALL_TIMEOUT_SECONDS}s.',
                                'error_details': {
                                    'type': 'TimeoutError', 
                                    'message': f'API call exceeded {API_CALL_TIMEOUT_SECONDS}s timeout',
                                    'actual_duration': api_response_time
                                },
                                'model': model, 'error_type': 'TimeoutError',
                                'api_response_time': api_response_time,
                                'attempts': attempt.retry_state.attempt_number
                            }
                        
                        # NÃ£o Ã© a Ãºltima tentativa - continua o loop de retry
                        raise
                    
                    except Exception as e:
                        # Outros erros da API
                        api_response_time = time.time() - start_time
                        error_details = {
                            'type': type(e).__name__, 
                            'message': str(e),
                            'actual_duration': api_response_time
                        }
                        
                        # Extrai headers se disponÃ­veis
                        response_headers = {}
                        if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                            response_headers = dict(e.response.headers)
                        
                        error_result = {
                            'id': request_id, 'success': False, 'error': str(e), 
                            'error_details': error_details, 'error_type': type(e).__name__, 
                            'model': model, 'api_response_time': api_response_time, 
                            'attempts': attempt.retry_state.attempt_number,
                            'response_headers': response_headers
                        }
                        
                        # Detecta rate limit de forma mais precisa
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            logger.warning(f"ğŸš¨ Rate limit detectado para {request_id}. Aguardando {wait_time}s antes de retry.")
                            self.rate_limiter.record_api_rate_limit(wait_time, batch_id)
                            # Re-raise para tentar novamente
                            raise
                        
                        # Para outros tipos de erro, verifica se deve fazer retry
                        retriable_errors = [
                            'InternalServerError', 'ServiceUnavailableError', 
                            'BadGatewayError', 'GatewayTimeoutError',
                            'ConnectionError', 'ConnectTimeoutError'
                        ]
                        
                        if type(e).__name__ in retriable_errors:
                            logger.warning(f"Erro retriÃ¡vel {type(e).__name__} para {request_id}. Tentativa {attempt.retry_state.attempt_number}/{self.config.max_retry}")
                            raise  # Re-raise para retry
                        
                        # Erro nÃ£o retriÃ¡vel - retorna imediatamente
                        return error_result
                        
        except RetryError as retry_error:
            # Esgotou todas as tentativas
            last_exception = retry_error.last_attempt.exception()
            final_error_details = {
                'type': type(last_exception).__name__,
                'message': str(last_exception),
                'max_retries_exceeded': True
            }
            
            return {
                'id': request_id, 'success': False, 
                'error': f'Falha apÃ³s {self.config.max_retry} tentativas: {str(last_exception)}',
                'error_details': final_error_details, 'model': model,
                'error_type': 'RetryError', 'attempts': self.config.max_retry,
                'api_response_time': 0.0  # NÃ£o hÃ¡ tempo especÃ­fico para retry error
            }
        
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)

    def _create_ordered_result(self, result: Dict[str, Any], start_timestamp_iso: str) -> Dict[str, Any]:
        result['start_timestamp'] = start_timestamp_iso
        base_fields = ['id', 'start_timestamp', 'success', 'model', 'api_response_time', 'attempts']
        ordered_result = {k: result.get(k) for k in base_fields}
        
        if result.get('success'):
            ordered_result.update({k: result.get(k) for k in ['input_tokens', 'output_tokens', 'cached_tokens', 'total_tokens', 'cost', 'content']})
        else:
            ordered_result.update({k: result.get(k) for k in ['error', 'error_details', 'error_type']})
        return ordered_result

    async def _process_and_record_result(self, result: Dict[str, Any], model: str, batch_id: Optional[str], remaining_tasks: int):
        result_for_stats = result.copy()
        result_for_stats.pop('model', None)

        # Registra no rate limiter com informaÃ§Ãµes mais detalhadas
        self.rate_limiter.record_request_completion(
            tokens_used=result.get('tokens_for_limiter', 0),
            success=result.get('success', False),
            latency=result.get('api_response_time', 0.0),
            batch_id=batch_id,
            remaining_tasks=remaining_tasks
        )
        
        # Registra no stats manager
        await self.stats_manager.record_request(
            batch_id=batch_id, 
            model=model,
            retry_count=max(0, result.get('attempts', 1) - 1), 
            **result_for_stats
        )

    async def _run_batch_processor(self, worker_coro: Callable, items: List, **kwargs):
        tasks = [asyncio.create_task(worker_coro(item, **kwargs)) for item in items]
        return await asyncio.gather(*tasks)

    def _get_final_batch_id(self, batch_id: Optional[str], prefix: str) -> str:
        unique_timestamp = int(time.time())
        return f"{batch_id}_{unique_timestamp}" if batch_id else f"{prefix}_batch_{unique_timestamp}"

    def _log_batch_completion(self, batch_id: str, batch_stats: stats_manager.StatsContainer):
        if not batch_stats: return
        
        # ObtÃ©m mÃ©tricas do rate limiter
        limiter_metrics = self.rate_limiter.get_current_metrics()
        
        summary = (
            f"âœ… Lote '{batch_id}' concluÃ­do. "
            f"DuraÃ§Ã£o: {batch_stats.processing_time:.2f}s | "
            f"Custo: ${batch_stats.total_cost:.4f} | "
            f"Tokens: {batch_stats.total_tokens:,} | "
            f"Sucesso: {batch_stats.successful_requests}/{batch_stats.total_requests} | "
            f"Pico de ConcorrÃªncia: {batch_stats.concurrent_peak} | "
            f"LatÃªncia MÃ©dia: {batch_stats.avg_api_latency:.2f}s | "
            f"EficiÃªncia Final: {limiter_metrics.get('efficiency_score', 0):.3f}"
        )
        
        extra_log = {
            'action': 'batch_complete', 'batch_id': batch_id,
            'processing_time': round(batch_stats.processing_time, 2),
            'total_cost': round(batch_stats.total_cost, 4),
            'total_requests': batch_stats.total_requests,
            'successful_requests': batch_stats.successful_requests,
            'avg_api_latency': round(batch_stats.avg_api_latency, 3),
            'concurrent_peak': batch_stats.concurrent_peak,
            'final_concurrency': limiter_metrics.get('concurrency', 0),
            'final_efficiency': round(limiter_metrics.get('efficiency_score', 0), 3),
            'final_success_rate': round(limiter_metrics.get('success_rate', 0), 3)
        }
        
        logger.info(summary, extra=extra_log)

    async def process_batch(self, texts: List[str], prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                            batch_id: Optional[str] = None, custom_ids: Optional[Any] = None, model: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "chat")
        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(f"ğŸš€ Iniciando lote de chat '{final_batch_id}' com {len(texts)} itens. ConcorrÃªncia inicial: {self.rate_limiter.dynamic_concurrency}")
        
        progress_tracker = self._BatchProgressTracker(len(texts), final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]
        
        results = await self._run_batch_processor(self._process_item_in_batch, items_to_process,
            prompt_template=prompt_template, json_schema=json_schema, batch_id=final_batch_id,
            progress_tracker=progress_tracker, model=model, **kwargs)
        
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    async def process_embedding_batch(self, texts: List[str], batch_id: Optional[str] = None,
                                      custom_ids: Optional[Any] = None, model: Optional[str] = None) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "embedding")
        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(f"ğŸš€ Iniciando lote de embedding '{final_batch_id}' com {len(texts)} itens. ConcorrÃªncia inicial: {self.rate_limiter.dynamic_concurrency}")
        
        progress_tracker = self._BatchProgressTracker(len(texts), final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]
        
        results = await self._run_batch_processor(self._process_embedding_item, items_to_process,
            batch_id=final_batch_id, progress_tracker=progress_tracker, model=model)
            
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    async def _process_item_in_batch(self, item_data: tuple, prompt_template: str, json_schema: Optional[Dict[str, Any]],
                                     batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None, **kwargs):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_req_{index}"
        
        prompt = prompt_template.format(text=text, **kwargs)
        api_model = model or self.config.model
        params = {"model": api_model, "messages": [{"role": "user", "content": prompt}], "temperature": self.config.temperature}
        if self.config.max_tokens: params["max_tokens"] = self.config.max_tokens
        if json_schema: params["response_format"] = {"type": "json_object"}
        
        result = await self._make_api_call(params, request_id, batch_id)
        
        await progress_tracker.increment_and_log(result.get('total_tokens', 0))
        await self._process_and_record_result(result, api_model, batch_id, progress_tracker.remaining)
        
        return self._create_ordered_result(result, start_timestamp_iso)

    async def _process_embedding_item(self, item_data: tuple, batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_emb_{index}"
        
        if self.embedding_cache is not None and text in self.embedding_cache:
            result = {'id': request_id, 'success': True, 'content': self.embedding_cache[text], 'model': 'cache', 'input_tokens': 0, 'output_tokens': 0, 
                      'cached_tokens': 0, 'total_tokens': 0, 'tokens_for_limiter': 0, 'cost': 0.0, 'api_response_time': 0.0, 'attempts': 0}
        else:
            api_model = model or self.config.embedding_model
            params = {"model": api_model, "input": text}
            result = await self._make_api_call(params, request_id, batch_id, is_embedding=True)
            if result.get('success') and self.embedding_cache is not None:
                self.embedding_cache[text] = result['content']
        
        await progress_tracker.increment_and_log(result.get('total_tokens', 0))
        await self._process_and_record_result(result, result.get('model', 'cache'), batch_id, progress_tracker.remaining)
        
        return self._create_ordered_result(result, start_timestamp_iso)

    # --- MÃ©todos para acessar relatÃ³rios de estatÃ­sticas ---

    def get_batch_report(self, batch_id: str) -> Optional[str]:
        """Retorna um relatÃ³rio de performance formatado para um batch_id especÃ­fico."""
        return self.stats_manager.get_batch_report(batch_id)

    def get_global_report(self) -> str:
        """Retorna um relatÃ³rio de performance global formatado."""
        return self.stats_manager.get_global_report()

    def get_batch_stats(self, batch_id: str) -> Optional[stats_manager.StatsContainer]:
        """Retorna o objeto StatsContainer bruto para um lote especÃ­fico."""
        return self.batch_stats_cache.get(batch_id)

    def get_global_stats(self) -> stats_manager.StatsContainer:
        """Retorna o objeto StatsContainer bruto para as estatÃ­sticas globais."""
        return self.stats_manager.get_global_stats()

    def get_rate_limiter_metrics(self) -> Dict[str, Any]:
        """Retorna mÃ©tricas atuais do rate limiter para debugging."""
        return self.rate_limiter.get_current_metrics()
