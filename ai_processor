import asyncio
import time
import json
import logging
import re
import os
import traceback
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Any, Optional, Callable, Union
from dataclasses import dataclass, field

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI  # Substitua pelo seu cliente de API real

from rate_limiter import AdaptiveRateLimiter
from stats_manager import StatsManager

logger = logging.getLogger(__name__)

# Fuso horÃ¡rio de referÃªncia (America/Sao_Paulo para HorÃ¡rio de BrasÃ­lia)
BR_TIMEZONE = ZoneInfo("America/Sao_Paulo")

# --- Constantes de ConfiguraÃ§Ã£o PadrÃ£o ---
DEFAULT_MAX_RETRY = 3
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TPM = 125000
DEFAULT_INITIAL_CONCURRENCY = 10

@dataclass
class AIProcessorConfig:
    """ConfiguraÃ§Ã£o validada para o AIProcessor."""
    client_id: str
    client_secret: str
    model: str = "gpt-4o-mini"  # Modelo padrÃ£o para chat
    embedding_model: str = "text-embedding-ada-002"  # Modelo padrÃ£o para embeddings
    temperature: float = DEFAULT_TEMPERATURE
    max_tokens: Optional[int] = None
    max_tpm: int = DEFAULT_MAX_TPM
    initial_concurrency: int = DEFAULT_INITIAL_CONCURRENCY
    max_retry: int = DEFAULT_MAX_RETRY
    environment: str = "dev"
    provider: str = "azure_openai"
    correlation_id: str = "ai-processor-final"
    enable_embedding_cache: bool = False  # Ativa cache simples para embeddings

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega a configuraÃ§Ã£o dos modelos a partir de um arquivo JSON no diretÃ³rio do mÃ³dulo."""
    # ObtÃ©m o diretÃ³rio onde este arquivo (ai_processor.py) estÃ¡ localizado
    module_dir = os.path.dirname(os.path.abspath(__file__))
    full_config_path = os.path.join(module_dir, config_path)
    
    if not os.path.exists(full_config_path):
        raise FileNotFoundError(f"Arquivo de configuraÃ§Ã£o '{full_config_path}' nÃ£o encontrado.")
    
    try:
        with open(full_config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        if not models_config or not isinstance(models_config, dict):
            raise ValueError(f"Arquivo '{full_config_path}' estÃ¡ vazio ou nÃ£o Ã© um dicionÃ¡rio JSON vÃ¡lido.")
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em '{full_config_path}': {e}")
    except Exception as e:
        raise RuntimeError(f"Erro inesperado ao carregar '{full_config_path}': {e}")

class JSONSchemaNotSupportedError(Exception):
    """ExceÃ§Ã£o levantada quando json_schema Ã© usado com um modelo incompatÃ­vel."""
    pass

class AIProcessor:
    """
    Orquestrador de chamadas para a API de IA, suportando chat completions e embeddings,
    com alta eficiÃªncia, robustez e visibilidade operacional.
    """

    class _BatchProgressTracker:
        """Classe interna para rastrear o progresso de um lote de forma atÃ´mica."""
        def __init__(self, total: int, batch_id: str, stats_manager: StatsManager):
            self.total = total
            self.batch_id = batch_id
            self.completed_count = 0
            self.total_tokens_used = 0
            self.start_time = time.time()
            self.logged_milestones = set()
            self._lock = asyncio.Lock()
            self._stats_manager = stats_manager

        async def increment_and_log(self, tokens_used: int = 0):
            """Incrementa o contador, adiciona tokens e loga progresso com mÃ©tricas de tokens."""
            async with self._lock:
                self.completed_count += 1
                self.total_tokens_used += tokens_used
                progress_percent = (self.completed_count / self.total) * 100
                current_milestone = int(progress_percent // 10) * 10
                
                if current_milestone > 0 and current_milestone not in self.logged_milestones:
                    self.logged_milestones.add(current_milestone)
                    elapsed = time.time() - self.start_time
                    req_rate = self.completed_count / elapsed if elapsed > 0 else 0
                    tpm_rate = (self.total_tokens_used / elapsed) * 60 if elapsed > 0 else 0
                    eta_seconds = (self.total - self.completed_count) / req_rate if req_rate > 0 else 0
                    
                    logger.info(
                        f"ðŸ“Š Progresso do Lote '{self.batch_id}': {self.completed_count}/{self.total} ({progress_percent:.1f}%) | "
                        f"Taxa Req: {req_rate:.2f}/s | Taxa TPM: {tpm_rate:.0f} | Tokens Usados: {self.total_tokens_used:,} | ETA: {eta_seconds/60:.1f} min",
                        extra={'action': 'batch_progress', 'batch_id': self.batch_id, 'completed': self.completed_count,
                               'total': self.total, 'progress_percent': round(progress_percent, 1),
                               'req_rate': round(req_rate, 2), 'tpm_rate': round(tpm_rate), 'total_tokens': self.total_tokens_used,
                               'eta_minutes': round(eta_seconds / 60, 1)}
                    )

    def __init__(self, config: AIProcessorConfig, api_client: AsyncIaraGenAI):
        self.models_config = load_models_config()
        self.supported_models = set(self.models_config.keys())
        
        self.config = config
        self.client = api_client  # InjeÃ§Ã£o de dependÃªncia
        
        if self.config.model not in self.supported_models:
            raise ValueError(f"Modelo '{self.config.model}' nÃ£o suportado. Modelos disponÃ­veis: {', '.join(sorted(self.supported_models))}")
        
        self.stats_manager = StatsManager(max_tpm=self.config.max_tpm)
        
        self.rate_limiter = AdaptiveRateLimiter(
            max_tpm=self.config.max_tpm,
            stats_callback=self._create_stats_callback(),
            initial_concurrency=self.config.initial_concurrency
        )
        
        self.embedding_cache: Dict[str, List[float]] = {} if self.config.enable_embedding_cache else None
        
        logger.info("AIProcessor inicializado com suporte a embeddings",
                    extra={'action': 'ai_processor_init', 'model': self.config.model, 'embedding_model': self.config.embedding_model,
                           'max_tpm': self.config.max_tpm, 'json_schema_supported': self._supports_json_schema(self.config.model)})

    def _get_model_pricing(self, model: str) -> Dict[str, float]:
        model_data = self.models_config.get(model, {})
        return {'input': model_data.get('input', 0.0), 'output': model_data.get('output', 0.0), 'cache': model_data.get('cache', 0.0)}

    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int = 0, cached_tokens: int = 0) -> float:
        pricing = self._get_model_pricing(model)
        regular_input = max(0, input_tokens - cached_tokens)
        return ((regular_input / 1000) * pricing['input'] + (cached_tokens / 1000) * pricing['cache'] + (output_tokens / 1000) * pricing['output'])

    def _create_stats_callback(self) -> Callable:
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback

    def _supports_json_schema(self, model: str) -> bool:
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]], model: str):
        if json_schema and not self._supports_json_schema(model):
            supported_models = [m for m, cfg in self.models_config.items() if cfg.get('json_schema')]
            raise JSONSchemaNotSupportedError(f"Modelo '{model}' nÃ£o suporta json_schema. Modelos compatÃ­veis: {', '.join(supported_models)}")

    @staticmethod
    def _is_rate_limit_error(result: Dict[str, Any]) -> bool:
        if not isinstance(result, dict): return False
        error_msg = result.get('error', '').lower()
        return 'token rate limit' in error_msg or 'rate limit' in error_msg

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try: return float(headers['retry-after'])
            except (ValueError, TypeError): pass
        error_msg = result.get('error', '').lower()
        match = re.search(r'try again in.*?(\d+\.?\d*)\s*s', error_msg)
        if match: return float(match.group(1))
        return 60.0

    @staticmethod
    def _normalize_ids(ids: Any, total_items: int) -> Optional[List[str]]:
        if ids is None: return None
        if hasattr(ids, 'tolist'): id_list = ids.tolist()
        elif hasattr(ids, 'values'): id_list = list(ids.values)
        elif not isinstance(ids, list): id_list = list(ids)
        else: id_list = ids
        if len(id_list) != total_items:
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(id_list)} != {total_items}")
        return [str(cid) if cid is not None else None for cid in id_list]

    def _get_current_timestamp_iso(self) -> str:
        """Gera timestamp formatado com milissegundos no fuso de BrasÃ­lia."""
        now = time.time()
        dt_object = datetime.fromtimestamp(now, tz=BR_TIMEZONE)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]

    async def _make_api_call(self, params: Dict[str, Any], request_id: str, batch_id: Optional[str] = None, is_embedding: bool = False) -> Dict[str, Any]:
        model = params.get('model', self.config.model)
        if is_embedding:
            self._validate_json_schema_compatibility(None, model)  # Embeddings nÃ£o usam schema
        else:
            self._validate_json_schema_compatibility(params.get('response_format'), model)
        
        await self.rate_limiter.await_permission_to_proceed(batch_id)
        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(self.config.max_retry), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    try:
                        if is_embedding:
                            response = await self.client.embeddings.create(**params)
                            input_tokens = response.usage.prompt_tokens
                            output_tokens = 0  # Embeddings nÃ£o geram output tokens
                            cached_tokens = 0
                            content = [emb.embedding for emb in response.data]  # Lista de vetores
                        else:
                            response = await self.client.chat.completions.create(**params)
                            input_tokens = response.usage.prompt_tokens
                            output_tokens = response.usage.completion_tokens
                            cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0)
                            content = response.choices[0].message.content
                            if params.get('response_format') and content:
                                try: content = json.loads(content)
                                except json.JSONDecodeError: pass
                        
                        cost = self._calculate_cost(model, input_tokens, output_tokens, cached_tokens)
                        
                        return {
                            'id': request_id, 'success': True, 'content': content,
                            'input_tokens': input_tokens, 'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens, 'total_tokens': input_tokens + output_tokens,
                            'cost': cost, 'api_response_time': time.time() - start_time,
                            'attempts': attempt.retry_state.attempt_number, 'raw_content': content if is_embedding else content
                        }
                    except Exception as e:
                        error_details = {'type': type(e).__name__, 'message': str(e), 'traceback': traceback.format_exc()}
                        error_result = {
                            'id': request_id, 'success': False, 'error': str(e), 'error_details': error_details,
                            'error_type': type(e).__name__, 'api_response_time': time.time() - start_time,
                            'attempts': attempt.retry_state.attempt_number,
                            'response_headers': dict(getattr(e.response, 'headers', {}) if hasattr(e, 'response') else {}),
                        }
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            self.rate_limiter.record_api_rate_limit(wait_time, batch_id)
                            await self.stats_manager.record_rate_limiter_event(
                                {'event_type': 'api_rate_limit_detected', 'wait_time': wait_time}, batch_id)
                            raise
                        return error_result
        except RetryError as retry_error:
            last_exception = retry_error.last_attempt.exception()
            error_details = {'type': type(last_exception).__name__, 'message': str(last_exception), 'traceback': traceback.format_exc()}
            return {
                'id': request_id, 'success': False, 'error': f'MÃ¡ximo de {self.config.max_retry} tentativas excedido.',
                'error_details': error_details, 'error_type': 'RetryError', 'attempts': self.config.max_retry,
            }
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)

    def _create_ordered_result(self, result: Dict[str, Any], start_timestamp_iso: str, is_embedding: bool = False) -> Dict[str, Any]:
        """Cria um dicionÃ¡rio de resultado final com a ordem de campos especificada."""
        base = {
            'id': result.get('id'), 'start_timestamp': start_timestamp_iso,
            'success': result.get('success'), 'api_response_time': result.get('api_response_time'),
            'attempts': result.get('attempts')
        }
        if result.get('success'):
            base.update({
                'input_tokens': result.get('input_tokens'),
                'output_tokens': result.get('output_tokens', 0),
                'cached_tokens': result.get('cached_tokens', 0),
                'total_tokens': result.get('total_tokens'),
                'cost': result.get('cost'),
                'content': result.get('content')  # Vetores para embeddings ou texto para chat
            })
            if not is_embedding:
                base['raw_content'] = result.get('raw_content')
        else:
            base.update({
                'error': result.get('error'),
                'error_details': result.get('error_details')
            })
        return base

    async def process_single(self, text: str, prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                             custom_id: Optional[str] = None, model: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Processa um Ãºnico texto via chat completion."""
        start_timestamp_iso = self._get_current_timestamp_iso()
        request_id = custom_id or f"req_{int(time.time() * 1000)}"
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        
        params = {"model": model or self.config.model, "messages": messages, "temperature": self.config.temperature}
        if self.config.max_tokens: params["max_tokens"] = self.config.max_tokens
        if json_schema: params["response_format"] = {"type": "json_object"}
        
        result = await self._make_api_call(params, request_id)
        
        self.rate_limiter.record_request_completion(result.get('total_tokens', 0), result.get('success', False))
        
        await self.stats_manager.record_request(model=params['model'], retry_count=max(0, result.get('attempts', 1) - 1), **result)
        
        return self._create_ordered_result(result, start_timestamp_iso)

    async def _process_item_in_batch(self, item_data, prompt_template: str, json_schema: Optional[Dict[str, Any]],
                                     batch_id: str, progress_tracker, model: Optional[str] = None, **kwargs):
        """FunÃ§Ã£o worker para chat completion em lote."""
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_req_{index}"
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        
        params = {"model": model or self.config.model, "messages": messages, "temperature": self.config.temperature}
        if self.config.max_tokens: params["max_tokens"] = self.config.max_tokens
        if json_schema: params["response_format"] = {"type": "json_object"}
        
        result = await self._make_api_call(params, request_id, batch_id)
        
        self.rate_limiter.record_request_completion(result.get('total_tokens', 0), result.get('success', False), batch_id)
        
        await self.stats_manager.record_request(batch_id=batch_id, model=params['model'], retry_count=max(0, result.get('attempts', 1) - 1), **result)
        
        await progress_tracker.increment_and_log(result.get('total_tokens', 0))
        
        return self._create_ordered_result(result, start_timestamp_iso)

    async def process_batch(self, texts: List[str], prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                            batch_id: Optional[str] = None, custom_ids: Optional[Any] = None, model: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Processa uma lista de textos em lote via chat completion."""
        unique_timestamp = int(time.time())
        batch_id = batch_id or f"batch_{unique_timestamp}"
        self.stats_manager.start_batch(batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(
            f"ðŸš€ Iniciando processamento em lote '{batch_id}' com {len(texts)} itens.",
            extra={'action': 'batch_start', 'batch_id': batch_id, 'total_items': len(texts)}
        )
        
        progress_tracker = self._BatchProgressTracker(total=len(texts), batch_id=batch_id, stats_manager=self.stats_manager)
        
        items_to_process = [
            (i, texts[i], normalized_ids[i] if normalized_ids else None)
            for i in range(len(texts))
        ]

        tasks = [
            asyncio.create_task(self._process_item_in_batch(item, prompt_template, json_schema, batch_id, progress_tracker, model=model, **kwargs))
            for item in items_to_process
        ]
        
        results = await asyncio.gather(*tasks)
        
        batch_stats = self.stats_manager.end_batch(batch_id)
        
        log_extra = {
            'action': 'batch_complete', 'batch_id': batch_id,
            'successful_requests': batch_stats.successful_requests,
            'failed_requests': batch_stats.failed_requests,
            'total_requests': batch_stats.total_requests,
            'processing_time_seconds': round(batch_stats.processing_time, 2),
            'start_time_iso': batch_stats.start_time_formatted,
            'end_time_iso': batch_stats.end_time_formatted,
            'total_cost': round(batch_stats.total_cost, 4)
        }
        logger.info(
            f"âœ… Lote '{batch_id}' concluÃ­do em {log_extra['processing_time_seconds']:.2f}s. "
            f"Sucesso: {batch_stats.successful_requests}, Falhas: {batch_stats.failed_requests}.",
            extra=log_extra
        )
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': batch_id}

    async def _process_embedding_item(self, item_data, batch_id: str, progress_tracker, model: Optional[str] = None):
        """FunÃ§Ã£o worker para embeddings em lote."""
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_emb_{index}"
        
        # Verifica cache se ativado
        if self.embedding_cache is not None and text in self.embedding_cache:
            cached_embedding = self.embedding_cache[text]
            result = {
                'id': request_id, 'success': True, 'content': cached_embedding,
                'input_tokens': 0, 'output_tokens': 0, 'cached_tokens': 0, 'total_tokens': 0,
                'cost': 0.0, 'api_response_time': 0.0, 'attempts': 0, 'from_cache': True
            }
        else:
            params = {"model": model or self.config.embedding_model, "input": text}
            result = await self._make_api_call(params, request_id, batch_id, is_embedding=True)
            
            if result.get('success') and self.embedding_cache is not None:
                self.embedding_cache[text] = result['content']
        
        self.rate_limiter.record_request_completion(result.get('
