import asyncio
import time
import json
import logging
import re
import os
import traceback
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI

import rate_limiter
import stats_manager

logger = logging.getLogger(__name__)

# Fuso horÃ¡rio de referÃªncia (America/Sao_Paulo para HorÃ¡rio de BrasÃ­lia)
BR_TIMEZONE = ZoneInfo("America/Sao_Paulo")

# --- Constantes de ConfiguraÃ§Ã£o PadrÃ£o ---
DEFAULT_MAX_RETRY = 3
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TPM = 1500000
DEFAULT_INITIAL_CONCURRENCY = 50

@dataclass
class AIProcessorConfig:
    """ConfiguraÃ§Ã£o validada para o AIProcessor."""
    client_id: str
    client_secret: str
    model: str = "gpt-4o-mini"
    embedding_model: str = "text-embedding-ada-002"
    temperature: float = DEFAULT_TEMPERATURE
    max_tokens: Optional[int] = None
    max_tpm: int = DEFAULT_MAX_TPM
    initial_concurrency: int = DEFAULT_INITIAL_CONCURRENCY
    max_retry: int = DEFAULT_MAX_RETRY
    environment: str = "dev"
    provider: str = "azure_openai"
    correlation_id: str = "ai-processor-final"
    enable_embedding_cache: bool = False

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega a configuraÃ§Ã£o dos modelos a partir de um arquivo JSON no diretÃ³rio do mÃ³dulo."""
    module_dir = os.path.dirname(os.path.abspath(__file__))
    full_config_path = os.path.join(module_dir, config_path)
    
    if not os.path.exists(full_config_path):
        raise FileNotFoundError(f"Arquivo de configuraÃ§Ã£o '{full_config_path}' nÃ£o encontrado.")
    
    try:
        with open(full_config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        if not models_config or not isinstance(models_config, dict):
            raise ValueError(f"Arquivo '{full_config_path}' estÃ¡ vazio ou nÃ£o Ã© um dicionÃ¡rio JSON vÃ¡lido.")
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em '{full_config_path}': {e}")
    except Exception as e:
        raise RuntimeError(f"Erro inesperado ao carregar '{full_config_path}': {e}")

class JSONSchemaNotSupportedError(Exception):
    """ExceÃ§Ã£o levantada quando json_schema Ã© usado com um modelo incompatÃ­vel."""
    pass

class AIProcessor:
    """
    Orquestrador de chamadas para a API de IA, suportando chat completions e embeddings,
    com alta eficiÃªncia, robustez e visibilidade operacional.
    """

    class _BatchProgressTracker:
        """Classe interna para rastrear o progresso de um lote de forma atÃ´mica."""
        def __init__(self, total: int, batch_id: str):
            self.total = total
            self.batch_id = batch_id
            self.completed_count = 0
            self.total_tokens_used = 0
            self.start_time = time.time()
            self.logged_milestones = set()
            self._lock = asyncio.Lock()

        async def increment_and_log(self, tokens_used: int = 0):
            """Incrementa o contador, adiciona tokens e loga progresso com mÃ©tricas de tokens."""
            async with self._lock:
                self.completed_count += 1
                self.total_tokens_used += tokens_used
                progress_percent = (self.completed_count / self.total) * 100
                current_milestone = int(progress_percent // 10) * 10
                
                if current_milestone > 0 and current_milestone not in self.logged_milestones:
                    self.logged_milestones.add(current_milestone)
                    elapsed = time.time() - self.start_time
                    req_rate = self.completed_count / elapsed if elapsed > 0 else 0
                    tpm_rate = (self.total_tokens_used / elapsed) * 60 if elapsed > 0 else 0
                    eta_seconds = (self.total - self.completed_count) / req_rate if req_rate > 0 else 0
                    
                    logger.info(
                        f"ðŸ“Š Progresso do Lote '{self.batch_id}': {self.completed_count}/{self.total} ({progress_percent:.1f}%) | "
                        f"Taxa Req: {req_rate:.2f}/s | Taxa TPM: {tpm_rate:,.0f} | Tokens Usados: {self.total_tokens_used:,} | ETA: {eta_seconds/60:.1f} min",
                        extra={'action': 'batch_progress', 'batch_id': self.batch_id, 'completed': self.completed_count,
                               'total': self.total, 'progress_percent': round(progress_percent, 1),
                               'req_rate': round(req_rate, 2), 'tpm_rate': round(tpm_rate), 'total_tokens': self.total_tokens_used,
                               'eta_minutes': round(eta_seconds / 60, 1)}
                    )

    def __init__(self, config: AIProcessorConfig):
        self.models_config = load_models_config()
        self.supported_models = set(self.models_config.keys())
        self.config = config
        
        self.client = AsyncIaraGenAI(
            client_id=self.config.client_id,
            client_secret=self.config.client_secret,
            environment=self.config.environment,
            provider=self.config.provider,
            correlation_id=self.config.correlation_id
        )
        
        if self.config.model not in self.supported_models:
            raise ValueError(f"Modelo '{self.config.model}' nÃ£o suportado. Modelos disponÃ­veis: {', '.join(sorted(self.supported_models))}")
        
        self.stats_manager = stats_manager.StatsManager(max_tpm=self.config.max_tpm)
        
        self.rate_limiter = rate_limiter.AdaptiveRateLimiter(
            max_tpm=self.config.max_tpm,
            stats_callback=self._create_stats_callback(),
            initial_concurrency=self.config.initial_concurrency
        )
        
        self.embedding_cache: Optional[Dict[str, List[float]]] = {} if self.config.enable_embedding_cache else None
        self.batch_stats_cache: Dict[str, stats_manager.StatsContainer] = {}
        
        # Cooldown para evitar spam de logs de pausa prÃ³-ativa
        self._last_proactive_pause_log_time = 0
        self.proactive_pause_log_cooldown_seconds = 5.0

        logger.info("AIProcessor inicializado",
                    extra={'action': 'ai_processor_init', 'model': self.config.model, 'embedding_model': self.config.embedding_model})

    def _get_model_pricing(self, model: str) -> Dict[str, float]:
        model_data = self.models_config.get(model, {})
        return {'input': model_data.get('input', 0.0), 'output': model_data.get('output', 0.0), 'cache': model_data.get('cache', 0.0)}

    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int = 0, cached_tokens: int = 0) -> float:
        pricing = self._get_model_pricing(model)
        regular_input = max(0, input_tokens - cached_tokens)
        return ((regular_input / 1000) * pricing['input'] + (cached_tokens / 1000) * pricing['cache'] + (output_tokens / 1000) * pricing['output'])

    def _create_stats_callback(self) -> Callable:
        """Cria um callback inteligente para o RateLimiter enviar eventos ao StatsManager."""
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            event_type = event.get('event_type')
            
            # Aplica cooldown no log de pausas prÃ³-ativas
            if event_type == 'proactive_pause':
                now = time.time()
                if (now - self._last_proactive_pause_log_time) > self.proactive_pause_log_cooldown_seconds:
                    self._last_proactive_pause_log_time = now
                    asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
            else:
                # Encaminha outros eventos normalmente
                asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback

    def _supports_json_schema(self, model: str) -> bool:
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]], model: str):
        if json_schema and not self._supports_json_schema(model):
            supported_models = [m for m, cfg in self.models_config.items() if cfg.get('json_schema')]
            raise JSONSchemaNotSupportedError(f"Modelo '{model}' nÃ£o suporta json_schema. Modelos compatÃ­veis: {', '.join(supported_models)}")

    @staticmethod
    def _is_rate_limit_error(result: Dict[str, Any]) -> bool:
        if not isinstance(result, dict): return False
        error_msg = result.get('error', '').lower()
        return 'token rate limit' in error_msg or 'rate limit' in error_msg

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try: return float(headers['retry-after'])
            except (ValueError, TypeError): pass
        error_msg = result.get('error', '').lower()
        match = re.search(r'try again in.*?(\d+\.?\d*)\s*s', error_msg)
        if match: return float(match.group(1))
        return 60.0

    @staticmethod
    def _normalize_ids(ids: Any, total_items: int) -> Optional[List[str]]:
        if ids is None: return None
        if hasattr(ids, 'tolist'): id_list = ids.tolist()
        elif hasattr(ids, 'values'): id_list = list(ids.values)
        elif not isinstance(ids, list): id_list = list(ids)
        else: id_list = ids
        if len(id_list) != total_items:
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(id_list)} != {total_items}")
        return [str(cid) if cid is not None else None for cid in id_list]

    def _get_current_timestamp_iso(self) -> str:
        now = time.time()
        dt_object = datetime.fromtimestamp(now, tz=BR_TIMEZONE)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]

    async def _make_api_call(self, params: Dict[str, Any], request_id: str, batch_id: Optional[str] = None, is_embedding: bool = False) -> Dict[str, Any]:
        model = params.get('model', self.config.model if not is_embedding else self.config.embedding_model)
        
        # A lÃ³gica para nÃ£o aumentar a concorrÃªncia se a latÃªncia estiver subindo
        # Ã© melhor implementada dentro do prÃ³prio AdaptiveRateLimiter, que agora considera
        # a latÃªncia como um sinal de saÃºde do sistema.
        await self.rate_limiter.await_permission_to_proceed(batch_id)
        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(self.config.max_retry), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    try:
                        if is_embedding:
                            response = await self.client.embeddings.create(**params)
                            content = response.data[0].embedding if isinstance(params.get("input"), str) else [emb.embedding for emb in response.data]
                            input_tokens, output_tokens, cached_tokens = response.usage.prompt_tokens, 0, 0
                        else:
                            self._validate_json_schema_compatibility(params.get('response_format'), model)
                            response = await self.client.chat.completions.create(**params)
                            content = response.choices[0].message.content
                            if params.get('response_format') and content:
                                try: content = json.loads(content)
                                except json.JSONDecodeError: pass
                            input_tokens, output_tokens = response.usage.prompt_tokens, response.usage.completion_tokens
                            cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0)
                        
                        api_response_time = time.time() - start_time
                        return {
                            'id': request_id, 'success': True, 'content': content,
                            'input_tokens': input_tokens, 'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens, 'total_tokens': input_tokens + output_tokens,
                            'tokens_for_limiter': (input_tokens - cached_tokens) + output_tokens,
                            'cost': self._calculate_cost(model, input_tokens, output_tokens, cached_tokens),
                            'api_response_time': api_response_time, 'attempts': attempt.retry_state.attempt_number
                        }
                    except Exception as e:
                        error_details = {'type': type(e).__name__, 'message': str(e), 'traceback': traceback.format_exc()}
                        error_result = {'id': request_id, 'success': False, 'error': str(e), 'error_details': error_details, 'error_type': type(e).__name__,
                                        'api_response_time': time.time() - start_time, 'attempts': attempt.retry_state.attempt_number,
                                        'response_headers': dict(getattr(e.response, 'headers', {}) if hasattr(e, 'response') else {})}
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            self.rate_limiter.record_api_rate_limit(wait_time, batch_id)
                            raise
                        return error_result
        except RetryError as retry_error:
            last_exception = retry_error.last_attempt.exception()
            error_details = {'type': type(last_exception).__name__, 'message': str(last_exception), 'traceback': traceback.format_exc()}
            return {'id': request_id, 'success': False, 'error': f'MÃ¡ximo de {self.config.max_retry} tentativas excedido.',
                    'error_details': error_details, 'error_type': 'RetryError', 'attempts': self.config.max_retry}
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)

    def _create_ordered_result(self, result: Dict[str, Any], start_timestamp_iso: str) -> Dict[str, Any]:
        ordered_result = {
            'id': result.get('id'),
            'start_timestamp': start_timestamp_iso,
            'success': result.get('success'),
            'api_response_time': result.get('api_response_time'),
            'attempts': result.get('attempts')
        }
        if result.get('success'):
            success_fields = ['input_tokens', 'output_tokens', 'cached_tokens', 'total_tokens', 'cost', 'content']
            ordered_result.update({k: result.get(k) for k in success_fields})
        else:
            error_fields = ['error', 'error_details']
            ordered_result.update({k: result.get(k) for k in error_fields})
        return ordered_result

    async def _process_and_record_result(self, result: Dict[str, Any], model: str, batch_id: Optional[str] = None):
        self.rate_limiter.record_request_completion(
            tokens_used=result.get('tokens_for_limiter', 0),
            success=result.get('success', False),
            latency=result.get('api_response_time', 0.0),
            batch_id=batch_id
        )
        await self.stats_manager.record_request(batch_id=batch_id, model=model, retry_count=max(0, result.get('attempts', 1) - 1), **result)

    async def _run_batch_processor(self, worker_coro: Callable, items: List, **kwargs):
        effective_concurrency = min(self.config.initial_concurrency, len(items))
        semaphore = asyncio.Semaphore(effective_concurrency)
        
        async def sem_worker(item):
            async with semaphore:
                return await worker_coro(item, **kwargs)

        tasks = [asyncio.create_task(sem_worker(item)) for item in items]
        return await asyncio.gather(*tasks)

    def _get_final_batch_id(self, batch_id: Optional[str], prefix: str) -> str:
        unique_timestamp = int(time.time())
        if batch_id:
            return f"{batch_id}_{unique_timestamp}"
        return f"{prefix}_batch_{unique_timestamp}"

    def _log_batch_completion(self, batch_id: str, batch_stats: stats_manager.StatsContainer):
        if not batch_stats:
            logger.error(f"NÃ£o foi possÃ­vel gerar log de conclusÃ£o para o lote '{batch_id}' pois as estatÃ­sticas nÃ£o foram encontradas.")
            return

        summary = (
            f"âœ… Lote '{batch_id}' concluÃ­do. "
            f"DuraÃ§Ã£o: {batch_stats.processing_time:.2f}s | "
            f"Custo Total: ${batch_stats.total_cost:.4f} | "
            f"Tokens Totais: {batch_stats.total_input_tokens + batch_stats.total_output_tokens:,} | "
            f"Sucesso: {batch_stats.successful_requests}/{batch_stats.total_requests}"
        )
        
        extra_log = {
            'action': 'batch_complete',
            'batch_id': batch_id,
            'processing_time': round(batch_stats.processing_time, 2),
            'total_cost': round(batch_stats.total_cost, 4),
            'total_requests': batch_stats.total_requests,
            'successful_requests': batch_stats.successful_requests,
            'failed_requests': batch_stats.failed_requests,
            'total_tokens': batch_stats.total_input_tokens + batch_stats.total_output_tokens,
            'avg_api_latency': round(batch_stats.avg_api_latency, 3),
            'concurrent_peak': batch_stats.concurrent_peak
        }
        logger.info(summary, extra=extra_log)

    async def process_batch(self, texts: List[str], prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                            batch_id: Optional[str] = None, custom_ids: Optional[Any] = None, model: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "chat")
        
        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(f"ðŸš€ Iniciando processamento de chat em lote '{final_batch_id}' com {len(texts)} itens.")
        progress_tracker = self._BatchProgressTracker(total=len(texts), batch_id=final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]

        results = await self._run_batch_processor(
            worker_coro=self._process_item_in_batch,
            items=items_to_process,
            prompt_template=prompt_template,
            json_schema=json_schema,
            batch_id=final_batch_id,
            progress_tracker=progress_tracker,
            model=model,
            **kwargs
        )
        
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)

        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    async def process_embedding_batch(self, texts: List[str], batch_id: Optional[str] = None,
                                      custom_ids: Optional[Any] = None, model: Optional[str] = None) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "embedding")

        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))

        logger.info(f"ðŸš€ Iniciando processamento de embedding em lote '{final_batch_id}' com {len(texts)} itens.")
        progress_tracker = self._BatchProgressTracker(total=len(texts), batch_id=final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]

        results = await self._run_batch_processor(
            worker_coro=self._process_embedding_item,
            items=items_to_process,
            batch_id=final_batch_id,
            progress_tracker=progress_tracker,
            model=model
        )
        
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)

        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    # --- MÃ©todos de acesso a estatÃ­sticas ---

    def get_batch_stats(self, batch_id: str) -> Optional[stats_manager.StatsContainer]:
        """Retorna o objeto de estatÃ­sticas para um lote especÃ­fico a partir do cache."""
        return self.batch_stats_cache.get(batch_id)

    def get_global_stats(self) -> stats_manager.StatsContainer:
        """Retorna o objeto de estatÃ­sticas globais."""
        return self.stats_manager.get_global_stats()

    # --- FunÃ§Ãµes worker internas para processamento em lote ---

    async def _process_item_in_batch(self, item_data: tuple, prompt_template: str, json_schema: Optional[Dict[str, Any]],
                                     batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None, **kwargs):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_req_{index}"
        prompt = prompt_template.format(text=text, **kwargs)
        
        api_model = model or self.config.model
        params = {"model": api_model, "messages": [{"role": "user", "content": prompt}], "temperature": self.config.temperature}
        if self.config.max_tokens: params["max_tokens"] = self.config.max_tokens
        if json_schema: params["response_format"] = {"type": "json_object"}
        
        result = await self._make_api_call(params, request_id, batch_id)
        await self._process_and_record_result(result, api_model, batch_id)
        await progress_tracker.increment_and_log(result.get('total_tokens', 0))
        return self._create_ordered_result(result, start_timestamp_iso)

    async def _process_embedding_item(self, item_data: tuple, batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_emb_{index}"
        
        if self.embedding_cache is not None and text in self.embedding_cache:
            result = {'id': request_id, 'success': True, 'content': self.embedding_cache[text], 'input_tokens': 0, 'output_tokens': 0, 
                      'cached_tokens': 0, 'total_tokens': 0, 'tokens_for_limiter': 0, 'cost': 0.0, 'api_response_time': 0.0, 'attempts': 0}
        else:
            api_model = model or self.config.embedding_model
            params = {"model": api_model, "input": text}
            result = await self._make_api_call(params, request_id, batch_id, is_embedding=True)
            if result.get('success') and self.embedding_cache is not None:
                self.embedding_cache[text] = result['content']
            await self._process_and_record_result(result, api_model, batch_id)

        await progress_tracker.increment_and_log(result.get('total_tokens', 0))
        return self._create_ordered_result(result, start_timestamp_iso)
