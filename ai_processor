import asyncio
import time
import json
import logging
import re
import os
import traceback
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI

import rate_limiter
import stats_manager

logger = logging.getLogger(__name__)

BR_TIMEZONE = ZoneInfo("America/Sao_Paulo")

DEFAULT_MAX_RETRY = 3
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TPM = 1500000
DEFAULT_INITIAL_CONCURRENCY = 30
# Timeout removido - ser√° capturado dinamicamente da API


@dataclass
class AIProcessorConfig:
    """Configura√ß√£o validada para o AIProcessor."""
    client_id: str
    client_secret: str
    model: str = "gpt-4o-mini"
    embedding_model: str = "text-embedding-ada-002"
    temperature: float = DEFAULT_TEMPERATURE
    max_tokens: Optional[int] = None
    max_tpm: int = DEFAULT_MAX_TPM
    initial_concurrency: int = DEFAULT_INITIAL_CONCURRENCY
    max_retry: int = DEFAULT_MAX_RETRY
    environment: str = "dev"
    provider: str = "azure_openai"
    correlation_id: str = "ai-processor-final"
    enable_embedding_cache: bool = False


def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega a configura√ß√£o dos modelos a partir de um arquivo JSON no diret√≥rio do m√≥dulo."""
    module_dir = os.path.dirname(os.path.abspath(__file__))
    full_config_path = os.path.join(module_dir, config_path)
    if not os.path.exists(full_config_path):
        raise FileNotFoundError(f"Arquivo de configura√ß√£o '{full_config_path}' n√£o encontrado.")
    try:
        with open(full_config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        raise RuntimeError(f"Erro ao carregar '{full_config_path}': {e}")


class JSONSchemaNotSupportedError(Exception):
    pass


class AIProcessor:
    class _BatchProgressTracker:
        def __init__(self, total: int, batch_id: str):
            self.total = total
            self.batch_id = batch_id
            self.completed_count = 0
            self.failed_count = 0
            self.total_tokens_used = 0
            self.start_time = time.time()
            self.logged_milestones = set()
            self._lock = asyncio.Lock()

        async def increment_and_log(self, tokens_used: int = 0, success: bool = True):
            async with self._lock:
                self.completed_count += 1
                if not success:
                    self.failed_count += 1
                self.total_tokens_used += tokens_used
                progress_percent = (self.completed_count / self.total) * 100
                
                current_milestone = int(progress_percent // 10) * 10
                if current_milestone > 0 and current_milestone not in self.logged_milestones:
                    self.logged_milestones.add(current_milestone)
                    elapsed = time.time() - self.start_time
                    req_rate = self.completed_count / elapsed if elapsed > 0 else 0
                    tpm_rate = (self.total_tokens_used / elapsed) * 60 if elapsed > 0 else 0
                    eta_seconds = (self.total - self.completed_count) / req_rate if req_rate > 0 else 0
                    success_rate = ((self.completed_count - self.failed_count) / self.completed_count) * 100
                    
                    logger.info(
                        f"üìä Progresso '{self.batch_id}': {self.completed_count}/{self.total} ({progress_percent:.1f}%) | "
                        f"Sucesso: {success_rate:.1f}% | Taxa: {req_rate:.2f}/s | TPM: {tpm_rate:,.0f} | ETA: {eta_seconds/60:.1f}min"
                    )
        
        @property
        def remaining(self) -> int:
            return self.total - self.completed_count

    def __init__(self, config: AIProcessorConfig):
        self.models_config = load_models_config()
        self.config = config
        self.client = AsyncIaraGenAI(
            client_id=config.client_id, client_secret=config.client_secret,
            environment=config.environment, provider=config.provider, correlation_id=config.correlation_id
        )
        self.stats_manager = stats_manager.StatsManager(max_tpm=config.max_tpm)
        self.rate_limiter = rate_limiter.AdaptiveRateLimiter(
            max_tpm=config.max_tpm,
            stats_callback=self._create_stats_callback(),
            initial_concurrency=config.initial_concurrency
        )
        self.embedding_cache = {} if config.enable_embedding_cache else None
        self.batch_stats_cache = {}

    def _create_stats_callback(self) -> Callable:
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback
        
    def _get_model_pricing(self, model: str) -> Dict[str, float]:
        model_data = self.models_config.get(model, {})
        return {'input': model_data.get('input', 0.0), 'output': model_data.get('output', 0.0), 'cache': model_data.get('cache', 0.0)}

    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int = 0, cached_tokens: int = 0) -> float:
        pricing = self._get_model_pricing(model)
        regular_input = max(0, input_tokens - cached_tokens)
        return ((regular_input / 1000) * pricing['input'] + (cached_tokens / 1000) * pricing['cache'] + (output_tokens / 1000) * pricing['output'])

    def _supports_json_schema(self, model: str) -> bool:
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]], model: str):
        if json_schema and not self._supports_json_schema(model):
            supported_models = [m for m, cfg in self.models_config.items() if cfg.get('json_schema')]
            raise JSONSchemaNotSupportedError(f"Modelo '{model}' n√£o suporta json_schema. Modelos compat√≠veis: {', '.join(supported_models)}")

    @staticmethod
    def _is_rate_limit_error(error_response: Dict[str, Any]) -> bool:
        """Detecta se o erro √© relacionado a rate limiting."""
        if not isinstance(error_response, dict):
            return False
        
        error_msg = str(error_response.get('error', '')).lower()
        error_details = error_response.get('error_details', {})
        error_type = str(error_details.get('type', '')).lower()
        
        rate_limit_patterns = [
            'rate limit', 'token rate limit', 'requests per minute',
            'too many requests', 'quota exceeded', 'ratelimiterror',
            'rate_limit_exceeded', 'throttled'
        ]
        
        text_to_check = f"{error_msg} {error_type}"
        return any(pattern in text_to_check for pattern in rate_limit_patterns)

    @staticmethod
    def _is_timeout_error_from_api(exception: Exception) -> bool:
        """Verifica se a exce√ß√£o √© um timeout reportado pela pr√≥pria API."""
        if not exception:
            return False
        
        error_message = str(exception).lower()
        error_type = type(exception).__name__.lower()
        
        # Padr√µes de timeout da API (n√£o nosso timeout local)
        api_timeout_patterns = [
            'timeout', 'timed out', 'request timeout',
            'gateway timeout', 'upstream timeout',
            'read timeout', 'connection timeout'
        ]
        
        # Verifica se √© timeout da API ou erro de rede com timeout
        return any(pattern in error_message for pattern in api_timeout_patterns)

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        """Extrai tempo de espera de headers ou mensagem de erro."""
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try:
                return float(headers['retry-after'])
            except (ValueError, TypeError):
                pass
        
        error_msg = result.get('error', '').lower()
        patterns = [
            r'try again in.*?(\d+\.?\d*)\s*s',
            r'retry after (\d+\.?\d*) seconds',
            r'wait (\d+\.?\d*) seconds'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, error_msg)
            if match:
                return float(match.group(1))
        
        if 'quota' in error_msg or 'daily' in error_msg:
            return 300.0
        elif 'minute' in error_msg:
            return 60.0
        else:
            return 30.0

    @staticmethod
    def _normalize_ids(ids: Any, total_items: int) -> Optional[List[str]]:
        if ids is None: return None
        if hasattr(ids, 'tolist'): id_list = ids.tolist()
        elif hasattr(ids, 'values'): id_list = list(ids.values)
        elif not isinstance(ids, list): id_list = list(ids)
        else: id_list = ids
        if len(id_list) != total_items:
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(id_list)} != {total_items}")
        return [str(cid) if cid is not None else None for cid in id_list]

    def _get_current_timestamp_iso(self) -> str:
        now = time.time()
        dt_object = datetime.fromtimestamp(now, tz=BR_TIMEZONE)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]

    async def _make_api_call(self, params: Dict[str, Any], request_id: str, batch_id: Optional[str] = None, is_embedding: bool = False) -> Dict[str, Any]:
        """
        Faz chamada √† API com tratamento robusto que NUNCA interrompe o lote.
        Todos os erros s√£o capturados e retornados como resultados de falha.
        """
        model = params.get('model', self.config.model if not is_embedding else self.config.embedding_model)
        
        await self.rate_limiter.await_permission_to_proceed(batch_id)
        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(self.config.max_retry), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    
                    try:
                        # Executa a chamada da API sem timeout artificial
                        if is_embedding:
                            response = await self.client.embeddings.create(**params)
                        else:
                            self._validate_json_schema_compatibility(params.get('response_format'), model)
                            response = await self.client.chat.completions.create(**params)
                        
                        # Processa resposta de sucesso
                        if is_embedding:
                            content = response.data[0].embedding if isinstance(params.get("input"), str) else [emb.embedding for emb in response.data]
                            input_tokens, output_tokens, cached_tokens = response.usage.prompt_tokens, 0, 0
                        else:
                            content = response.choices[0].message.content
                            if params.get('response_format') and content:
                                try: content = json.loads(content)
                                except json.JSONDecodeError: pass
                            input_tokens, output_tokens = response.usage.prompt_tokens, response.usage.completion_tokens
                            cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0)
                        
                        return {
                            'id': request_id, 'success': True, 'content': content, 'model': model,
                            'input_tokens': input_tokens, 'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens, 'total_tokens': input_tokens + output_tokens,
                            'tokens_for_limiter': (input_tokens - cached_tokens) + output_tokens,
                            'cost': self._calculate_cost(model, input_tokens, output_tokens, cached_tokens),
                            'api_response_time': time.time() - start_time,
                            'attempts': attempt.retry_state.attempt_number
                        }
                        
                    except Exception as e:
                        api_response_time = time.time() - start_time
                        error_details = {
                            'type': type(e).__name__,
                            'message': str(e),
                            'actual_duration': api_response_time
                        }
                        
                        response_headers = {}
                        if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                            response_headers = dict(e.response.headers)
                        
                        error_result = {
                            'id': request_id, 'success': False, 'error': str(e),
                            'error_details': error_details, 'error_type': type(e).__name__,
                            'model': model, 'api_response_time': api_response_time,
                            'attempts': attempt.retry_state.attempt_number,
                            'response_headers': response_headers
                        }
                        
                        # Detecta e trata rate limits
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            logger.warning(f"üö® Rate limit detectado para {request_id}. Aguardando {wait_time}s")
                            self.rate_limiter.record_api_rate_limit(wait_time, batch_id)
                            raise  # Re-raise para retry
                        
                        # Detecta timeouts da API (n√£o nossos)
                        if self._is_timeout_error_from_api(e):
                            logger.warning(f"‚è±Ô∏è Timeout da API detectado para {request_id}. Tentativa {attempt.retry_state.attempt_number}/{self.config.max_retry}")
                            if attempt.retry_state.attempt_number < self.config.max_retry:
                                raise  # Re-raise para retry
                        
                        # Erros retri√°veis (server-side)
                        retriable_errors = [
                            'InternalServerError', 'ServiceUnavailableError',
                            'BadGatewayError', 'GatewayTimeoutError',
                            'ConnectionError', 'ConnectTimeoutError',
                            'ReadTimeoutError', 'HTTPError'
                        ]
                        
                        if type(e).__name__ in retriable_errors:
                            logger.warning(f"üîÑ Erro retri√°vel {type(e).__name__} para {request_id}. Tentativa {attempt.retry_state.attempt_number}/{self.config.max_retry}")
                            if attempt.retry_state.attempt_number < self.config.max_retry:
                                raise  # Re-raise para retry
                        
                        # Se chegou aqui, √© erro n√£o retri√°vel ou √∫ltima tentativa
                        logger.error(f"‚ùå Erro definitivo para {request_id}: {type(e).__name__} - {str(e)}")
                        return error_result
                        
        except RetryError as retry_error:
            # CR√çTICO: Captura RetryError e converte em resultado de falha
            # Isso evita que o lote seja interrompido
            last_exception = retry_error.last_attempt.exception()
            api_response_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            logger.error(f"üíÄ Falha definitiva ap√≥s {self.config.max_retry} tentativas para {request_id}: {type(last_exception).__name__}")
            
            return {
                'id': request_id, 'success': False,
                'error': f'Falha ap√≥s {self.config.max_retry} tentativas: {str(last_exception)}',
                'error_details': {
                    'type': type(last_exception).__name__,
                    'message': str(last_exception),
                    'max_retries_exceeded': True,
                    'final_attempt_duration': api_response_time
                },
                'model': model, 'error_type': 'RetryError',
                'attempts': self.config.max_retry,
                'api_response_time': api_response_time
            }
        
        except Exception as unexpected_error:
            # CR√çTICO: Captura qualquer exce√ß√£o n√£o prevista
            # Garante que o lote continue mesmo com erros inesperados
            logger.error(f"üö® Erro inesperado para {request_id}: {type(unexpected_error).__name__} - {str(unexpected_error)}")
            logger.error(f"üîç Traceback: {traceback.format_exc()}")
            
            return {
                'id': request_id, 'success': False,
                'error': f'Erro inesperado: {str(unexpected_error)}',
                'error_details': {
                    'type': type(unexpected_error).__name__,
                    'message': str(unexpected_error),
                    'unexpected_error': True,
                    'traceback': traceback.format_exc()
                },
                'model': model, 'error_type': 'UnexpectedError',
                'attempts': 1, 'api_response_time': 0.0
            }
        
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)

    def _create_ordered_result(self, result: Dict[str, Any], start_timestamp_iso: str) -> Dict[str, Any]:
        result['start_timestamp'] = start_timestamp_iso
        base_fields = ['id', 'start_timestamp', 'success', 'model', 'api_response_time', 'attempts']
        ordered_result = {k: result.get(k) for k in base_fields}
        
        if result.get('success'):
            ordered_result.update({k: result.get(k) for k in ['input_tokens', 'output_tokens', 'cached_tokens', 'total_tokens', 'cost', 'content']})
        else:
            ordered_result.update({k: result.get(k) for k in ['error', 'error_details', 'error_type']})
        return ordered_result

    async def _process_and_record_result(self, result: Dict[str, Any], model: str, batch_id: Optional[str], remaining_tasks: int):
        result_for_stats = result.copy()
        result_for_stats.pop('model', None)

        self.rate_limiter.record_request_completion(
            tokens_used=result.get('tokens_for_limiter', 0),
            success=result.get('success', False),
            latency=result.get('api_response_time', 0.0),
            batch_id=batch_id,
            remaining_tasks=remaining_tasks
        )
        
        await self.stats_manager.record_request(
            batch_id=batch_id,
            model=model,
            retry_count=max(0, result.get('attempts', 1) - 1),
            **result_for_stats
        )

    async def _run_batch_processor(self, worker_coro: Callable, items: List, **kwargs):
        """
        Executa o processamento em lote com garantia de conclus√£o.
        Usa gather com return_exceptions=True para n√£o falhar em exce√ß√µes individuais.
        """
        tasks = [asyncio.create_task(worker_coro(item, **kwargs)) for item in items]
        # CR√çTICO: return_exceptions=True evita que uma exce√ß√£o pare todo o lote
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Processa resultados e converte exce√ß√µes em resultados de falha
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Converte exce√ß√£o em resultado de falha
                item_data = items[i]
                request_id = f"task_{i}"
                if isinstance(item_data, tuple) and len(item_data) >= 3:
                    request_id = item_data[2] or request_id
                
                logger.error(f"üö® Exce√ß√£o n√£o capturada na task {i}: {type(result).__name__} - {str(result)}")
                
                error_result = {
                    'id': request_id, 'success': False,
                    'error': f'Exce√ß√£o na task: {str(result)}',
                    'error_details': {
                        'type': type(result).__name__,
                        'message': str(result),
                        'task_exception': True
                    },
                    'model': 'unknown', 'error_type': 'TaskException',
                    'attempts': 0, 'api_response_time': 0.0,
                    'start_timestamp': self._get_current_timestamp_iso()
                }
                processed_results.append(error_result)
            else:
                processed_results.append(result)
        
        return processed_results

    def _get_final_batch_id(self, batch_id: Optional[str], prefix: str) -> str:
        unique_timestamp = int(time.time())
        return f"{batch_id}_{unique_timestamp}" if batch_id else f"{prefix}_batch_{unique_timestamp}"

    def _log_batch_completion(self, batch_id: str, batch_stats: stats_manager.StatsContainer):
        if not batch_stats: return
        
        limiter_metrics = self.rate_limiter.get_current_metrics()
        failed_requests = batch_stats.total_requests - batch_stats.successful_requests
        
        summary = (
            f"‚úÖ Lote '{batch_id}' CONCLU√çDO (100% processado). "
            f"Dura√ß√£o: {batch_stats.processing_time:.2f}s | "
            f"Custo: ${batch_stats.total_cost:.4f} | "
            f"Tokens: {batch_stats.total_tokens:,} | "
            f"Sucesso: {batch_stats.successful_requests}/{batch_stats.total_requests}"
        )
        
        if failed_requests > 0:
            summary += f" | ‚ùå Falhas: {failed_requests}"
        
        summary += (
            f" | Pico: {batch_stats.concurrent_peak} | "
            f"Lat√™ncia: {batch_stats.avg_api_latency:.2f}s | "
            f"Efici√™ncia: {limiter_metrics.get('efficiency_score', 0):.3f}"
        )
        
        extra_log = {
            'action': 'batch_complete', 'batch_id': batch_id,
            'processing_time': round(batch_stats.processing_time, 2),
            'total_cost': round(batch_stats.total_cost, 4),
            'total_requests': batch_stats.total_requests,
            'successful_requests': batch_stats.successful_requests,
            'failed_requests': failed_requests,
            'success_rate': round((batch_stats.successful_requests / batch_stats.total_requests) * 100, 2),
            'avg_api_latency': round(batch_stats.avg_api_latency, 3),
            'concurrent_peak': batch_stats.concurrent_peak,
            'final_concurrency': limiter_metrics.get('concurrency', 0),
            'final_efficiency': round(limiter_metrics.get('efficiency_score', 0), 3)
        }
        
        logger.info(summary, extra=extra_log)

    async def process_batch(self, texts: List[str], prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                            batch_id: Optional[str] = None, custom_ids: Optional[Any] = None, model: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "chat")
        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(f"üöÄ Iniciando lote ROBUSTO '{final_batch_id}' com {len(texts)} itens. Concorr√™ncia: {self.rate_limiter.dynamic_concurrency}")
        
        progress_tracker = self._BatchProgressTracker(len(texts), final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]
        
        # Executa com garantia de conclus√£o
        results = await self._run_batch_processor(self._process_item_in_batch, items_to_process,
            prompt_template=prompt_template, json_schema=json_schema, batch_id=final_batch_id,
            progress_tracker=progress_tracker, model=model, **kwargs)
        
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    async def process_embedding_batch(self, texts: List[str], batch_id: Optional[str] = None,
                                      custom_ids: Optional[Any] = None, model: Optional[str] = None) -> Dict[str, Any]:
        final_batch_id = self._get_final_batch_id(batch_id, "embedding")
        self.stats_manager.start_batch(final_batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(f"üöÄ Iniciando lote ROBUSTO de embedding '{final_batch_id}' com {len(texts)} itens. Concorr√™ncia: {self.rate_limiter.dynamic_concurrency}")
        
        progress_tracker = self._BatchProgressTracker(len(texts), final_batch_id)
        items_to_process = [(i, texts[i], (normalized_ids[i] if normalized_ids else None)) for i in range(len(texts))]
        
        results = await self._run_batch_processor(self._process_embedding_item, items_to_process,
            batch_id=final_batch_id, progress_tracker=progress_tracker, model=model)
        
        batch_stats = self.stats_manager.end_batch(final_batch_id)
        if batch_stats:
            self.batch_stats_cache[final_batch_id] = batch_stats
            self._log_batch_completion(final_batch_id, batch_stats)
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': final_batch_id}

    async def _process_item_in_batch(self, item_data: tuple, prompt_template: str, json_schema: Optional[Dict[str, Any]],
                                     batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None, **kwargs):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_req_{index}"
        
        try:
            prompt = prompt_template.format(text=text, **kwargs)
            api_model = model or self.config.model
            params = {"model": api_model, "messages": [{"role": "user", "content": prompt}], "temperature": self.config.temperature}
            if self.config.max_tokens: params["max_tokens"] = self.config.max_tokens
            if json_schema: params["response_format"] = {"type": "json_object"}
            
            result = await self._make_api_call(params, request_id, batch_id)
            
            await progress_tracker.increment_and_log(result.get('total_tokens', 0), result.get('success', False))
            await self._process_and_record_result(result, api_model, batch_id, progress_tracker.remaining)
            
            return self._create_ordered_result(result, start_timestamp_iso)
            
        except Exception as e:
            # Captura qualquer exce√ß√£o n√£o tratada no n√≠vel do item
            logger.error(f"üö® Erro inesperado processando item {request_id}: {type(e).__name__} - {str(e)}")
            
            error_result = {
                'id': request_id, 'success': False,
                'error': f'Erro no processamento do item: {str(e)}',
                'error_details': {
                    'type': type(e).__name__,
                    'message': str(e),
                    'item_processing_error': True
                },
                'model': model or self.config.model,
                'error_type': 'ItemProcessingError',
                'attempts': 0, 'api_response_time': 0.0
            }
            
            await progress_tracker.increment_and_log(0, False)
            await self._process_and_record_result(error_result, model or self.config.model, batch_id, progress_tracker.remaining)
            
            return self._create_ordered_result(error_result, start_timestamp_iso)

    async def _process_embedding_item(self, item_data: tuple, batch_id: str, progress_tracker: _BatchProgressTracker, model: Optional[str] = None):
        start_timestamp_iso = self._get_current_timestamp_iso()
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_emb_{index}"
        
        try:
            if self.embedding_cache is not None and text in self.embedding_cache:
                result = {'id': request_id, 'success': True, 'content': self.embedding_cache[text], 'model': 'cache', 'input_tokens': 0, 'output_tokens': 0,
                          'cached_tokens': 0, 'total_tokens': 0, 'tokens_for_limiter': 0, 'cost': 0.0, 'api_response_time': 0.0, 'attempts': 0}
            else:
                api_model = model or self.config.embedding_model
                params = {"model": api_model, "input": text}
                result = await self._make_api_call(params, request_id, batch_id, is_embedding=True)
                if result.get('success') and self.embedding_cache is not None:
                    self.embedding_cache[text] = result['content']
            
            await progress_tracker.increment_and_log(result.get('total_tokens', 0), result.get('success', False))
            await self._process_and_record_result(result, result.get('model', 'cache'), batch_id, progress_tracker.remaining)
            
            return self._create_ordered_result(result, start_timestamp_iso)
            
        except Exception as e:
            logger.error(f"üö® Erro inesperado processando embedding {request_id}: {type(e).__name__} - {str(e)}")
            
            error_result = {
                'id': request_id, 'success': False,
                'error': f'Erro no processamento do embedding: {str(e)}',
                'error_details': {
                    'type': type(e).__name__,
                    'message': str(e),
                    'embedding_processing_error': True
                },
                'model': model or self.config.embedding_model,
                'error_type': 'EmbeddingProcessingError',
                'attempts': 0, 'api_response_time': 0.0
            }
            
            await progress_tracker.increment_and_log(0, False)
            await self._process_and_record_result(error_result, model or self.config.embedding_model, batch_id, progress_tracker.remaining)
            
            return self._create_ordered_result(error_result, start_timestamp_iso)

    # --- M√©todos para acessar relat√≥rios de estat√≠sticas ---

    def get_batch_report(self, batch_id: str) -> Optional[str]:
        """Retorna um relat√≥rio de performance formatado para um batch_id espec√≠fico."""
        return self.stats_manager.get_batch_report(batch_id)

    def get_global_report(self) -> str:
        """Retorna um relat√≥rio de performance global formatado."""
        return self.stats_manager.get_global_report()

    def get_batch_stats(self, batch_id: str) -> Optional[stats_manager.StatsContainer]:
        """Retorna o objeto StatsContainer bruto para um lote espec√≠fico."""
        return self.batch_stats_cache.get(batch_id)

    def get_global_stats(self) -> stats_manager.StatsContainer:
        """Retorna o objeto StatsContainer bruto para as estat√≠sticas globais."""
        return self.stats_manager.get_global_stats()

    def get_rate_limiter_metrics(self) -> Dict[str, Any]:
        """Retorna m√©tricas atuais do rate limiter para debugging."""
        return self.rate_limiter.get_current_metrics()
