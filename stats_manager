import asyncio
import time
import logging
import statistics
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger(__name__)

BR_TIMEZONE = ZoneInfo("America/Sao_Paulo")

@dataclass
class StatsContainer:
    """
    Um cont√™iner de dados robusto e otimizado para armazenar e calcular m√©tricas
    de desempenho, com timestamps formatados e insights de paraleliza√ß√£o.
    """
    # --- Identifica√ß√£o e Tempo ---
    id: str
    max_tpm_limit: int = 0
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None

    # --- Contadores de Requisi√ß√£o ---
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    error_type_counts: Counter = field(default_factory=Counter)
    total_retry_count: int = 0

    # --- Contadores de Tokens e Custo ---
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cached_tokens: int = 0
    total_cost: float = 0.0

    # --- Rastreamento de Lat√™ncia ---
    api_response_times: List[float] = field(default_factory=list)

    # --- Rastreamento de Concorr√™ncia ---
    current_concurrent_requests: int = 0
    concurrent_peak: int = 0

    # --- M√©tricas de Rate Limiter ---
    api_rate_limits_detected: int = 0
    proactive_pauses: int = 0
    total_proactive_wait_time: float = 0.0
    peak_tpm: int = 0

    # --- Propriedades Calculadas ---
    @property
    def total_tokens(self) -> int:
        return self.total_input_tokens + self.total_output_tokens
        
    @property
    def processing_time(self) -> float:
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

    @property
    def total_api_time(self) -> float:
        return sum(self.api_response_times)

    @property
    def parallelization_gain_seconds(self) -> float:
        processing_time = self.processing_time
        if processing_time > 0 and self.total_api_time > processing_time:
            return self.total_api_time - processing_time
        return 0.0

    @property
    def parallelization_gain_percent(self) -> float:
        if self.total_api_time > 0:
            return (self.parallelization_gain_seconds / self.total_api_time) * 100
        return 0.0
        
    @property
    def min_api_latency(self) -> float:
        return min(self.api_response_times) if self.api_response_times else 0

    @property
    def max_api_latency(self) -> float:
        return max(self.api_response_times) if self.api_response_times else 0

    @property
    def avg_api_latency(self) -> float:
        return statistics.mean(self.api_response_times) if self.api_response_times else 0

    @property
    def requests_per_second(self) -> float:
        processing_time = self.processing_time
        if processing_time > 0:
            return self.total_requests / processing_time
        return 0.0

    # --- IN√çCIO DA CORRE√á√ÉO ---
    # M√©todos de formata√ß√£o e apresenta√ß√£o do relat√≥rio.
    
    @staticmethod
    def _format_timestamp(timestamp: Optional[float]) -> str:
        if not timestamp: return "N/A"
        dt_object = datetime.fromtimestamp(timestamp, tz=BR_TIMEZONE)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]

    @property
    def start_time_formatted(self) -> str:
        return self._format_timestamp(self.start_time)

    @property
    def end_time_formatted(self) -> str:
        return self._format_timestamp(self.end_time)

    def get_report(self) -> str:
        """Formata as estat√≠sticas em um relat√≥rio leg√≠vel e rico em insights."""
        title = f" RELAT√ìRIO DE PERFORMANCE: {self.id.upper()} "
        header = f"\n{title.center(80, '=')}"

        summary = (
            f"üìä RESUMO GERAL:\n"
            f"   - Per√≠odo de Execu√ß√£o: {self.start_time_formatted} -> {self.end_time_formatted}\n"
            f"   - Dura√ß√£o Total: {self.processing_time:.3f}s\n"
            f"   - Requisi√ß√µes: {self.total_requests} (‚úÖ {self.successful_requests} Sucesso | ‚ùå {self.failed_requests} Falhas)\n"
            f"   - Custo Total Estimado: ${self.total_cost:.4f}\n"
            f"   - Total de Tokens: {self.total_tokens:,} (Entrada: {self.total_input_tokens:,}, Sa√≠da: {self.total_output_tokens:,})"
        )

        parallelization = ""
        if self.total_requests > 1:
            parallelization = (
                f"\nüöÄ EFICI√äNCIA DA PARALELIZA√á√ÉO:\n"
                f"   - Tempo Total de API (Sequencial): {self.total_api_time:.3f}s\n"
                f"   - Ganho com Paraleliza√ß√£o: {self.parallelization_gain_seconds:.3f}s ({self.parallelization_gain_percent:.1f}% de economia de tempo)\n"
                f"   - Pico de Concorr√™ncia: {self.concurrent_peak} requisi√ß√µes simult√¢neas"
            )

        tpm_usage_percent = (self.peak_tpm / self.max_tpm_limit * 100) if self.max_tpm_limit > 0 else 0
        performance = (
            f"\n‚è±Ô∏è PERFORMANCE E LAT√äNCIA:\n"
            f"   - Vaz√£o (Throughput): {self.requests_per_second:.2f} reqs/s\n"
            f"   - Lat√™ncia da API (m√©dia | min | max): {self.avg_api_latency:.3f}s | {self.min_api_latency:.3f}s | {self.max_api_latency:.3f}s\n"
            f"   - Pico de TPM: {self.peak_tpm:,} ({tpm_usage_percent:.1f}% do limite de {self.max_tpm_limit:,})\n"
            f"   - Pausas Proativas: {self.proactive_pauses} (totalizando {self.total_proactive_wait_time:.2f}s)"
        )
        
        errors = ""
        if self.failed_requests > 0:
            error_details = ', '.join([f"{k}: {v}" for k, v in self.error_type_counts.items()])
            errors = (
                f"\n‚ö†Ô∏è AN√ÅLISE DE ERROS:\n"
                f"   - Total de Falhas: {self.failed_requests}\n"
                f"   - Tipos de Erro: {error_details}\n"
                f"   - Rate Limits da API Detectados: {self.api_rate_limits_detected}"
            )

        footer = "=" * 80
        return "\n".join(filter(None, [header, summary, parallelization, performance, errors, footer]))

# --- FIM DA CORRE√á√ÉO ---

class StatsManager:
    def __init__(self, max_tpm: int):
        self.max_tpm = max_tpm
        self._global_stats = StatsContainer(id="global", max_tpm_limit=self.max_tpm)
        self._batch_stats: Dict[str, StatsContainer] = {}
        self._lock = asyncio.Lock()
        logger.info("StatsManager inicializado e pronto para gerar insights.")

    async def _update_stats(self, stats: StatsContainer, **kwargs):
        stats.total_requests += 1
        if kwargs.get('success', False):
            stats.successful_requests += 1
            stats.total_cost += kwargs.get('cost', 0.0)
            stats.total_input_tokens += kwargs.get('input_tokens', 0)
            stats.total_output_tokens += kwargs.get('output_tokens', 0)
            stats.total_cached_tokens += kwargs.get('cached_tokens', 0)
        else:
            stats.failed_requests += 1
            stats.error_type_counts[kwargs.get('error_type', 'UnknownError')] += 1
        
        if (api_time := kwargs.get('api_response_time', 0.0)) > 0:
            stats.api_response_times.append(api_time)
        stats.total_retry_count += kwargs.get('retry_count', 0)

    async def record_request(self, batch_id: Optional[str] = None, **kwargs):
        async with self._lock:
            await self._update_stats(self._global_stats, **kwargs)
            if batch_id and batch_id in self._batch_stats:
                await self._update_stats(self._batch_stats[batch_id], **kwargs)

    async def record_rate_limiter_event(self, event: Dict[str, Any], batch_id: Optional[str] = None):
        event_type = event.get('event_type')
        async with self._lock:
            for stats in self._get_relevant_containers(batch_id):
                if event_type == 'proactive_pause':
                    stats.proactive_pauses += 1
                    stats.total_proactive_wait_time += event.get('wait_time', 0.0)
                elif event_type == 'api_rate_limit_detected':
                     stats.api_rate_limits_detected += 1
                elif event_type == 'token_usage_update':
                    current_tpm = event.get('current_tpm', 0)
                    if current_tpm > stats.peak_tpm:
                        stats.peak_tpm = current_tpm
    
    async def record_concurrent_start(self, batch_id: Optional[str] = None):
        async with self._lock:
            for stats in self._get_relevant_containers(batch_id):
                stats.current_concurrent_requests += 1
                if stats.current_concurrent_requests > stats.concurrent_peak:
                    stats.concurrent_peak = stats.current_concurrent_requests

    async def record_concurrent_end(self, batch_id: Optional[str] = None):
        async with self._lock:
            for stats in self._get_relevant_containers(batch_id):
                stats.current_concurrent_requests = max(0, stats.current_concurrent_requests - 1)

    def _get_relevant_containers(self, batch_id: Optional[str]) -> List[StatsContainer]:
        containers = [self._global_stats]
        if batch_id and batch_id in self._batch_stats:
            containers.append(self._batch_stats[batch_id])
        return containers

    def start_batch(self, batch_id: str):
        if batch_id in self._batch_stats:
            logger.warning(f"Batch com ID '{batch_id}' j√° existe. Sobrescrevendo estat√≠sticas.")
        self._batch_stats[batch_id] = StatsContainer(id=batch_id, max_tpm_limit=self.max_tpm)
        logger.info(f"Lote de estat√≠sticas '{batch_id}' iniciado.")

    def end_batch(self, batch_id: str) -> Optional[StatsContainer]:
        if batch_stats := self._batch_stats.get(batch_id):
            batch_stats.end_time = time.time()
            return batch_stats
        return None

    def get_global_stats(self) -> StatsContainer:
        return self._global_stats

    # --- IN√çCIO DA CORRE√á√ÉO ---
    # M√©todos p√∫blicos para obter os relat√≥rios formatados.

    def get_batch_report(self, batch_id: str) -> Optional[str]:
        """Gera e retorna um relat√≥rio formatado para um lote espec√≠fico."""
        if batch_stats := self._batch_stats.get(batch_id):
            return batch_stats.get_report()
        logger.warning(f"Nenhuma estat√≠stica encontrada para o batch ID '{batch_id}'.")
        return None

    def get_global_report(self) -> str:
        """Gera e retorna um relat√≥rio formatado para as estat√≠sticas globais."""
        self._global_stats.end_time = time.time() # Garante que o tempo de processamento global esteja atualizado
        return self._global_stats.get_report()
    # --- FIM DA CORRE√á√ÉO ---
