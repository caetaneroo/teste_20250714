import asyncio
import time
import logging
from collections import deque
from typing import Deque, Tuple, Callable, Dict, Any

logger = logging.getLogger(__name__)

# --- Constantes de Configuração para o Rate Limiter Dinâmico ---
WINDOW_SECONDS = 60.0
METRICS_HISTORY_SIZE = 100
DEFAULT_PREDICTED_COST = 1500
TPM_TARGET_FACTOR = 0.90
ADJUSTMENT_COOLDOWN_SECONDS = 5.0
# Fator para determinar se a latência está aumentando de forma preocupante
LATENCY_INCREASE_THRESHOLD_FACTOR = 1.5

class AdaptiveRateLimiter:
    """
    Um governador de vazão que gerencia dinamicamente a concorrência e a taxa de
    requisições, considerando tanto o limite de TPM quanto a latência da API.
    """

    def __init__(
        self,
        max_tpm: int,
        stats_callback: Callable[[Dict[str, Any], Any], None],
        initial_concurrency: int
    ):
        if max_tpm <= 0:
            raise ValueError("max_tpm deve ser um valor positivo.")

        self.effective_max_tpm = max_tpm
        self._stats_callback = stats_callback
        self._lock = asyncio.Lock()

        # --- Estado do Controle de Fluxo ---
        self.token_usage_window: Deque[Tuple[float, int]] = deque()
        self.tokens_in_window: int = 0
        self.recent_request_costs: Deque[int] = deque(maxlen=METRICS_HISTORY_SIZE)
        self.recent_latencies: Deque[float] = deque(maxlen=METRICS_HISTORY_SIZE)
        self._avg_request_cost: float = DEFAULT_PREDICTED_COST
        self._avg_latency: float = 1.0  # Inicia com um valor base

        # --- Estado do Controle de Concorrência ---
        self._semaphore = asyncio.Semaphore(initial_concurrency)
        self.dynamic_concurrency = initial_concurrency
        self._is_adjusting_semaphore = False
        self._last_adjustment_time: float = 0.0
        self._last_proactive_pause_log_time: float = 0.0

        logger.info(
            "AdaptiveRateLimiter (Throughput Governor) inicializado",
            extra={'action': 'rate_limiter_init', 'max_tpm': self.effective_max_tpm,
                   'initial_concurrency': self.dynamic_concurrency}
        )

    def _prune_usage_window(self):
        """Remove registros de tokens da janela que são mais antigos que WINDOW_SECONDS."""
        now = time.time()
        while self.token_usage_window and (now - self.token_usage_window[0][0] > WINDOW_SECONDS):
            _timestamp, tokens = self.token_usage_window.popleft()
            self.tokens_in_window -= tokens

    async def await_permission_to_proceed(self, batch_id: Any = None) -> None:
        """Aguarda permissão para executar, adquirindo um slot do semáforo e verificando o TPM."""
        await self._semaphore.acquire()
        
        async with self._lock:
            self._prune_usage_window()
            target_tpm = self.effective_max_tpm * TPM_TARGET_FACTOR
            if self.tokens_in_window >= target_tpm:
                now = time.time()
                wait_time = (WINDOW_SECONDS - (now - self.token_usage_window[0][0])) if self.token_usage_window else 0
                
                if wait_time > 0:
                    # Aplica cooldown para não spammar o log
                    if (now - self._last_proactive_pause_log_time) > ADJUSTMENT_COOLDOWN_SECONDS:
                        logger.warning(
                            f"Limite de TPM se aproximando. Pausa proativa de {wait_time:.2f}s.",
                            extra={'action': 'proactive_pause', 'wait_time': wait_time}
                        )
                        self._last_proactive_pause_log_time = now
                    self._stats_callback({'event_type': 'proactive_pause', 'wait_time': wait_time}, batch_id)
                    await asyncio.sleep(wait_time)

    def record_request_completion(self, tokens_used: int, success: bool, latency: float, batch_id: Any = None):
        """Registra o resultado e libera o slot do semáforo, agora aceitando latência."""
        try:
            self._semaphore.release()
        except ValueError:
            pass # Ignora erro se a capacidade for reduzida
        asyncio.create_task(self._async_process_result(tokens_used, success, latency, batch_id))

    async def _async_process_result(self, tokens_used: int, success: bool, latency: float, batch_id: Any):
        """Processa o resultado, atualiza métricas e ajusta a concorrência."""
        async with self._lock:
            if success:
                if tokens_used > 0:
                    now = time.time()
                    self.token_usage_window.append((now, tokens_used))
                    self.tokens_in_window += tokens_used
                    self.recent_request_costs.append(tokens_used)
                    self._avg_request_cost = sum(self.recent_request_costs) / len(self.recent_request_costs)
                
                if latency > 0:
                    self.recent_latencies.append(latency)
                    self._avg_latency = sum(self.recent_latencies) / len(self.recent_latencies)

            self._prune_usage_window()
            current_tpm_in_window = self.tokens_in_window
        
        self._stats_callback({'event_type': 'token_usage_update', 'current_tpm': current_tpm_in_window}, batch_id)
        await self._adjust_concurrency_strategy(latency)

    async def _adjust_concurrency_strategy(self, current_latency: float):
        """Define a nova estratégia de concorrência com base no TPM e na latência."""
        if self._is_adjusting_semaphore or (time.time() - self._last_adjustment_time < ADJUSTMENT_COOLDOWN_SECONDS):
            return

        if self._avg_request_cost <= 0:
            return

        # Lógica de controle de concorrência baseada em latência
        if current_latency > self._avg_latency * LATENCY_INCREASE_THRESHOLD_FACTOR:
            # Latência está subindo, sinal de saturação. Não aumenta a concorrência.
            logger.warning(
                f"Latência da API ({current_latency:.2f}s) acima da média ({self._avg_latency:.2f}s). "
                "Pausando aumento de concorrência para evitar sobrecarga.",
                extra={'action': 'latency_backoff'}
            )
            return

        target_tpm = self.effective_max_tpm * TPM_TARGET_FACTOR
        ideal_concurrency = int(target_tpm / self._avg_request_cost)
        ideal_concurrency = max(1, ideal_concurrency)

        await self._set_concurrency(ideal_concurrency)

    async def _set_concurrency(self, new_capacity: int):
        """Ajusta a capacidade do semáforo de forma atômica e segura."""
        self._is_adjusting_semaphore = True
        try:
            async with self._lock:
                if new_capacity == self.dynamic_concurrency:
                    return

                logger.info(
                    f"Ajustando nível de concorrência de {self.dynamic_concurrency} para {new_capacity}",
                    extra={'action': 'concurrency_adjustment', 'old_concurrency': self.dynamic_concurrency, 'new_concurrency': new_capacity}
                )
                
                diff = new_capacity - self.dynamic_concurrency
                if diff > 0:
                    for _ in range(diff): self._semaphore.release()
                elif diff < 0:
                    tasks = [asyncio.create_task(self._semaphore.acquire()) for _ in range(abs(diff))]
                    await asyncio.gather(*tasks)

                self.dynamic_concurrency = new_capacity
                self._last_adjustment_time = time.time()
                self._stats_callback({'event_type': 'concurrency_update', 'new_concurrency': self.dynamic_concurrency}, None)
        finally:
            self._is_adjusting_semaphore = False

    def record_api_rate_limit(self, wait_time: float, batch_id: Any = None):
        """Ativa o freio de emergência ao detectar um erro de rate limit da API."""
        asyncio.create_task(self._async_handle_rate_limit_error(wait_time, batch_id))

    async def _async_handle_rate_limit_error(self, wait_time: float, batch_id: Any):
        """Processa o erro de rate limit, reduzindo drasticamente a concorrência."""
        logger.error(f"RATE LIMIT DA API DETECTADO! Concorrência será reduzida. Pausando por {wait_time:.1f}s.")
        new_concurrency = max(1, int(self.dynamic_concurrency / 2))
        await self._set_concurrency(new_concurrency)
        self._stats_callback({'event_type': 'api_rate_limit_detected', 'wait_time': wait_time}, batch_id)
